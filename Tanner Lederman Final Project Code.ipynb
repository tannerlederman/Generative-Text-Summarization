{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import re\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk import sent_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Attention\n",
    "from keras.layers import Embedding\n",
    "import tensorflow as tf\n",
    "import pydot\n",
    "#from attention_decoder import AttentionDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Instructions: \n",
    " 1. Make sure the json folder (containing all the .json files) and the document \"oa-ccby-40k-ids.csv\" are in the same folder at the same level as this jupyter notebook\n",
    " 2. Run from top to bottom!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator for loading in and getting the vocabulary set for each level\n",
    "CORPUS_SIZE = 750\n",
    "CORPUS_SIZE_WITH_TESTING = 1000\n",
    "def data_generator_overall_vocab(papers_ids):\n",
    "    json_file_ids = pd.read_csv(papers_ids)\n",
    "    file_ids = np.array(json_file_ids)\n",
    "    X_batch, y_batch = [],[]\n",
    "    X_vocab = []\n",
    "    X_sentences = []\n",
    "    index = 0\n",
    "    while True:\n",
    "        X_batch, y_batch = [], []\n",
    "        s = \"json/\" + str(file_ids[index][0]) + \".json\"\n",
    "        data = json.load(open(s))\n",
    "        sentences = []\n",
    "        # grabs all the sentences in the body text\n",
    "        for key, value in data.items():\n",
    "            if key == \"body_text\":\n",
    "                for v in value:\n",
    "                    sentences.append(v[\"sentence\"])\n",
    "        sentences = np.array(sentences)\n",
    "        #preprocessing\n",
    "        # makes everything lowercase and removes punctuation\n",
    "        for i in range(0, sentences.size):\n",
    "            sentences[i] = sentences[i].lower()\n",
    "            sentences[i] = re.sub(r'[^\\w\\s]', '', sentences[i])\n",
    "        X_sentences = np.array([sentences[i] for i in range(0, sentences.size)])\n",
    "        # transform sentences into a list of words\n",
    "        X_batch = np.array([np.array(sentences[i].split(\" \")) for i in range(0, sentences.size)])\n",
    "        try:\n",
    "            # get the abstract data\n",
    "            y_sentences = sent_tokenize(data[\"abstract\"])\n",
    "            for i in range(0, len(y_sentences)):\n",
    "                # preprocessing\n",
    "                y_sentences[i] = y_sentences[i].lower()\n",
    "                y_sentences[i] = re.sub(r'[^\\w\\s]', '', y_sentences[i])\n",
    "            # splits each sentence  into a list of words\n",
    "            y_batch = np.array([np.array(sentence.split(\" \")) for sentence in y_sentences])\n",
    "        except:\n",
    "            yield\n",
    "        # gets all the words used\n",
    "        X_corpus = []\n",
    "        for l in X_batch:\n",
    "            for t in l:\n",
    "                X_corpus.append(t)\n",
    "        index = index + 1\n",
    "        # np.unique(X_corpus) returns the vocabulary set in this document\n",
    "        yield X_batch, y_batch, X_sentences, np.unique(X_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_generator = data_generator_overall_vocab(\"oa-ccby-40k-ids.csv\")\n",
    "# X_train- the data in the form of np.array(np.array(np.array)) where the layers are document-sentence-word from out to in\n",
    "# y_train - the data is in the same format as X_train\n",
    "# documents - the data is in the form of np.array(np.array()) where the layers are document-sentence\n",
    "# sentence_dump - a list of all the sentences in the corpus\n",
    "X_train, y_train, documents, sentence_dump = np.empty(CORPUS_SIZE, dtype = object), np.empty(CORPUS_SIZE, dtype = object), np.empty(CORPUS_SIZE, dtype = object), []\n",
    "X_test, y_test, documents_test, sentence_dump_test = np.empty(CORPUS_SIZE_WITH_TESTING - CORPUS_SIZE, dtype = object), np.empty(CORPUS_SIZE_WITH_TESTING - CORPUS_SIZE, dtype = object), np.empty(CORPUS_SIZE_WITH_TESTING - CORPUS_SIZE, dtype = object), []\n",
    "\n",
    "# iterates through the given document\n",
    "# params: index- document id\n",
    "# start_v: the current set of vocabulary before processing this document\n",
    "#\n",
    "def iterate(index, start_v):\n",
    "    x_b, y_b, x_s, v = next(vocab_generator)\n",
    "    X_train[index] = x_b\n",
    "    y_train[index] = y_b\n",
    "    documents[index] = x_s\n",
    "    for s in x_s:\n",
    "        sentence_dump.append(s)\n",
    "    return np.union1d(start_v, v)\n",
    "\n",
    "def iterate_test(index):\n",
    "    x_b, y_b, x_s, v = next(vocab_generator)\n",
    "    X_test[index] = x_b\n",
    "    y_test[index] = y_b\n",
    "    documents_test[index] = x_s\n",
    "    for s in x_s:\n",
    "        sentence_dump_test.append(s)\n",
    "    pass\n",
    "# goes through all the data in the corpus\n",
    "x_1, y_1, x_s1, vocab = next(vocab_generator)\n",
    "X_train[0] = x_1\n",
    "y_train[0] = y_1\n",
    "documents[0] = x_s1\n",
    "num_fails = 0\n",
    "for i in range(1,CORPUS_SIZE):\n",
    "    try:\n",
    "        vocab = iterate(i, vocab)\n",
    "    except:\n",
    "        continue\n",
    "for i in range(0, CORPUS_SIZE_WITH_TESTING - CORPUS_SIZE):\n",
    "    try:\n",
    "        iterate(i,vocab)\n",
    "    except:\n",
    "        continue\n",
    "# after running this cell, X_train, y_train, documents, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets rid of the null training documents\n",
    "index = 0\n",
    "for X in X_train:\n",
    "    X = np.array(X)\n",
    "    if X.size <= 1:\n",
    "        X_train = np.delete(X_train, i)\n",
    "    else:\n",
    "        index += 1\n",
    "REFINED_CORPUS_SIZE = X_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds zeros of embedding size to the vocab words not in the current model\n",
    "def fill_in_blanks(vocab, word2vec_model):\n",
    "    for v in vocab:\n",
    "        try:\n",
    "            word2vec_model.wv[v]\n",
    "        except:\n",
    "            word2vec_model.wv[v] = np.zeros(200)\n",
    "    return word2vec_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF approach to weighting the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Manual Approach For Attention Vector\n",
    "# gets the overall counts of all the documents in the corpus\n",
    "def word_count_dict(vocab, data):\n",
    "    count_dict = {}\n",
    "    for v in vocab:\n",
    "        count_dict[v] = 0\n",
    "    index = 0\n",
    "    for X in data:\n",
    "        X = np.array(X)\n",
    "        if X.size <= 1:\n",
    "            continue\n",
    "        for arr in X:\n",
    "            arr = np.array(arr)\n",
    "            for token in arr:\n",
    "                try:\n",
    "                    count_dict[token] = count_dict[token] + 1\n",
    "                except:\n",
    "                    continue\n",
    "        index += 1\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the frequency of all terms in the selected paper\n",
    "def term_frequency(counter, data, index):\n",
    "    term_dict = {}\n",
    "    total_count = 0\n",
    "    if data[index] == None:\n",
    "        return term_dict\n",
    "    for arr in data[index]:\n",
    "        arr = np.array(arr)\n",
    "        for token in arr:\n",
    "            total_count = total_count + 1\n",
    "            \n",
    "    for c in counter:\n",
    "        term_dict[c] = counter[c] / total_count\n",
    "    \n",
    "    return term_dict, total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the log inverse document appearance of a all tokens in the document\n",
    "def inverse_term_frequency(counter, data):\n",
    "    inverse_dict = {}\n",
    "    for c in counter.keys():\n",
    "        inDoc = 0\n",
    "        for doc in data:\n",
    "            if doc == None:\n",
    "                continue\n",
    "            if any(c in x for x in np.array(doc)):\n",
    "                inDoc += 1\n",
    "            # to smooth the data, if it does not occur, say it occurred once\n",
    "        if inDoc == 0:\n",
    "            inverse_dict[c] == math.log(REFINED_CORPUS_SIZE/1)\n",
    "        else:\n",
    "            inverse_dict[c] = math.log(REFINED_CORPUS_SIZE / inDoc)\n",
    "\n",
    "    return inverse_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the overall tf_idf weights for the words in the vocabulary\n",
    "def tf_idf(vocab, data, index):\n",
    "    tf_idf = {}\n",
    "    counter = word_count_dict(vocab, data)\n",
    "    term_freq = term_frequency(counter, data, index)\n",
    "    inverse_term_freq = inverse_term_frequency(counter, data)\n",
    "    for term in vocab:\n",
    "        tf_idf[term] = term_freq[term] * inverse_term_freq[term]\n",
    "    return td_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MergeSort for getting top 10,000 words, as I was getting a truth value error that I could not find a fix for in the built-in functions\n",
    "def merge(l, r):\n",
    "    n = len(l) + len(r)\n",
    "    A = l\n",
    "    for key, value in r.items():\n",
    "        A[key] = value \n",
    "    keys_A = list(A.keys())\n",
    "    keys_r = list(r.keys())\n",
    "    keys_l = list(l.keys())\n",
    "    j = 0\n",
    "    k = 0\n",
    "    for i in range(0, n):\n",
    "        if (j > len(l)):\n",
    "            keys_A[i] = keys_r[k]\n",
    "            A[keys_A[i]] = r[keys_r[k]]\n",
    "            k += 1\n",
    "        elif (k > len(l)):\n",
    "            keys_A[i] = keys_l[j]\n",
    "            A[keys_A[i]] = l[keys_l[j]]\n",
    "            j += 1\n",
    "        elif (l[keys_l[j]] <= r[keys_r[k]]):\n",
    "            keys_A[i] = keys_l[j]\n",
    "            A[keys_A[i]] = l[keys_l[j]]\n",
    "            j += 1\n",
    "        else:\n",
    "            keys_A[i] = keys_r[k]\n",
    "            A[keys_A[i]] = r[keys_r[k]]\n",
    "            k += 1\n",
    "    return A\n",
    "            \n",
    "# trims the vocab down to the top ten thousand words\n",
    "# uses a MergeSort Algorithm\n",
    "def trimVocab(counter):\n",
    "    if (len(counter) == 1):\n",
    "        return counter\n",
    "    right_side = dict(list(counter.items())[len(counter)//2:])\n",
    "    left_side = dict(list(counter.items())[:len(counter)//2])\n",
    "    left_side = trimVocab(left_side)\n",
    "    right_side = trimVocab(right_side)\n",
    "    \n",
    "    counter = merge(left_side, right_side)\n",
    "    ten_thousand_most_common = dict(list(counter.items())[:10000])\n",
    "    return ten_thousand_most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the counter dictionary (not a Counter object, wasn't working as well in other functions)\n",
    "counter = word_count_dict(vocab,X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# alternative method that I realized worked after implementing MergeSort\n",
    "# quicker than my implementation, so I switched it over\n",
    "res = dict(list(sorted(counter.items(), key = lambda x: x[1], reverse = True))[:10000])\n",
    "res_k = list(res.keys())\n",
    "trimmed_vocab = list(res.keys())\n",
    "trimmed_vocab.append(\"UNK\")\n",
    "trimmed_vocab = np.array(trimmed_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts the number of words in a given Document\n",
    "def docCount(data, index):\n",
    "    total_count = 0\n",
    "    if data[index] == None:\n",
    "        return 0\n",
    "    for arr in data[index]:\n",
    "        arr = np.array(arr)\n",
    "        for token in arr:\n",
    "            total_count += 1\n",
    "    \n",
    "    return total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps the word to index and vice versa, for converting words to a numerical categorical value\n",
    "def mapWordToIndex(vocab):\n",
    "    w_t_i = {}\n",
    "    i_t_w = {}\n",
    "    w_t_i[\"UNK\"] = 0\n",
    "    i_t_w[0] = \"UNK\"\n",
    "    index = 1\n",
    "    for v in vocab:\n",
    "        w_t_i[v] = index\n",
    "        i_t_w[index] = v\n",
    "        index += 1\n",
    "        \n",
    "    return w_t_i, i_t_w\n",
    "\n",
    "# maps the word and indices from mapWordToIndex to the Word2Vec Embeddings\n",
    "def mapToEmbedding(i_t_w, word2vec, vocab_size):\n",
    "    i_t_e = {}\n",
    "    w_t_e = {}\n",
    "    for i, w in i_t_w.items():\n",
    "        if w == \"UNK\":\n",
    "            i_t_e[i] = np.zeros(200)\n",
    "            w_t_e[w] = np.zeros(200)\n",
    "        else:\n",
    "            i_t_e[i] = word2vec.wv[w]\n",
    "            w_t_e[w] = word2vec.wv[w]\n",
    "    return i_t_e, w_t_e\n",
    "\n",
    "# creates an embedding matrix of size (vocab_size, embedding_size)\n",
    "# needed to put in the embedding_intializer parameter in the keras Embedding Layer for the RNN\n",
    "def createEmbeddingMatrix(vocab_size, embedding_size, i_t_e):\n",
    "    embedding_matrix = np.zeros((vocab_size + 1, embedding_size))\n",
    "    for i, e in i_t_e.items():\n",
    "        embedding_matrix[i] = e\n",
    "    return embedding_matrix    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepared\n"
     ]
    }
   ],
   "source": [
    "# splits all the sentences into a simple 2d array to process in word2vec\n",
    "split_sentences = np.array([sentence.split(\" \") for sentence in sentence_dump])\n",
    "word2vec_model = Word2Vec(sentences = split_sentences, sg = 1, window = 5, size = 200, min_count = 1)\n",
    "word2vec_model = fill_in_blanks(vocab, word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10002, 200)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use word2vec as a model input\n",
    "w_t_i, i_t_w = mapWordToIndex(trimmed_vocab)\n",
    "i_t_e, w_t_e = mapToEmbedding(i_t_w, word2vec_model, trimmed_vocab.size)\n",
    "embedding_matrix = createEmbeddingMatrix(trimmed_vocab.size, 200, i_t_e)\n",
    "#word2vec_model = Word2Vec(sentences = sentences_dump, sg = 1, window = 5, size = 200, min_count = 1)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "### Pointer Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for RNN, where most of the action happens\n",
    "# for each individual document\n",
    "def add_new_word(word_data, cur_sum, inputs_src, pos):\n",
    "    vocab_size = trimmed_vocab.size + 1\n",
    "    # Average abstract length is 150-250 words in length, so I thought 200 words would be a good length.\n",
    "    sum_txt_length = pos\n",
    "    # source side for Hidden Layer W\n",
    "    \n",
    "    # overloaded my application memory even with one epoch\n",
    "    src_embedding = Embedding(vocab_size, 200, embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix), trainable=False)(inputs_src)\n",
    "    src_hidden_layer = LSTM(200)(src_embedding)\n",
    "    #sum side for Hidden Layer U\n",
    "    \n",
    "    # did not use pre-trained word embeddings as this is supposed to take into account the already used words in the summary \n",
    "    inputs_cur_sum = Input(shape=(sum_txt_length,))\n",
    "    cur_sum_embedding = Embedding(vocab_size, 200)(inputs_cur_sum)\n",
    "    cur_sum_hidden_layer = LSTM(200)(cur_sum_embedding)\n",
    "    #decoder side for Hidden Layer V\n",
    "    attention_result = Attention()([src_hidden_layer, cur_sum_hidden_layer])\n",
    "    decoder = tf.concat([attention_result, cur_sum_hidden_layer], 1)\n",
    "    decoded = Dense(vocab_size, activation='softmax')(decoder)\n",
    "    \n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for the RNN, though most of the work is done here\n",
    "def add_new_word_simple(word_data, cur_sum, inputs_src, pos):\n",
    "    vocab_size = trimmed_vocab.size + 1\n",
    "    # Average abstract length is 150-250 words in length, so I thought 200 words would be a good length.\n",
    "    sum_txt_length = pos\n",
    "    \n",
    "    # source side for Hidden Layer W\n",
    "    src_embedding = Embedding(vocab_size, 200, embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix), trainable=False)(inputs_src)\n",
    "    src_hidden_layer = LSTM(200)(src_embedding)\n",
    "    decoded = Dense(vocab_size, activation='sigmoid')(src_hidden_layer)\n",
    "    #attention layer\n",
    "    # the distribution is the TF-IDF for this document\n",
    "    \n",
    "#     attention_dist = tf_idf(trimmed_vocab, X_train, 0)\n",
    "#    attention_result = Attention()([decoded, ])\n",
    "    simple_model = Model(inputs=inputs_src, outputs = decoded)\n",
    "\n",
    "    simple_model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that creates the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creaes the RNN, compiles it, and returns the model\n",
    "def document_summarize(sum_length, article_choice, simple):\n",
    "    model = Model()\n",
    "    inputs_src = Input(shape=(7000,))\n",
    "    cur_sum = Input(shape=(None,))\n",
    "    output_sum = cur_sum\n",
    "    if simple:\n",
    "        output_sum = tf.concat([output_sum,add_new_word_simple(X_train, output_sum, inputs_src, i)], axis = 1)\n",
    "    else:\n",
    "        output_sum = add_new_word(X_train, output_sum, inputs_src, i)\n",
    "    if simple:\n",
    "        model = Model(inputs=inputs_src, outputs = output_sum)\n",
    "    else:\n",
    "        sum_len = Input(shape=(sum_length,))\n",
    "        model = Model(inputs=[inputs_src,cur_sum], outputs = output_sum)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next cell throws an error as the models do not compile properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"input_3:0\", shape=(None, 999), dtype=float32) at layer \"embedding_1\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d65ef00ccee9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument_summarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-20bfb9f56145>\u001b[0m in \u001b[0;36mdocument_summarize\u001b[0;34m(sum_length, article_choice, simple)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0msum_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs_src\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcur_sum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m       \u001b[0;31m# Functional model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m       \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunctional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name, trainable)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m#     'arguments during initialization. Got an unexpected argument:')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFunctional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     nodes, nodes_by_depth, layers, _ = _map_graph_network(\n\u001b[0;32m--> 191\u001b[0;31m         self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    192\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m    929\u001b[0m                              \u001b[0;34m'The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m                              \u001b[0;34m'were accessed without issue: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                              str(layers_with_complete_input))\n\u001b[0m\u001b[1;32m    932\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m           \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"input_3:0\", shape=(None, 999), dtype=float32) at layer \"embedding_1\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "# Pointer Generator\n",
    "model = document_summarize(20, 0, False)\n",
    "simple_model = document_summarize(20,0,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generates the batch data for the step in the epoch of training the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateStepper(X_train, y_train, vocab, simple):\n",
    "    index = 0\n",
    "    # will pass the data as indices\n",
    "    w_t_i, i_t_w = mapWordToIndex(vocab)\n",
    "    while True:\n",
    "        while (X_train[index] == None) or (y_train[index] == None):\n",
    "            index+=1\n",
    "        X_start = X_train[index]\n",
    "        X_batch = []\n",
    "        src_txt_length = docCount(X_train, index)\n",
    "        req_length = 0\n",
    "        # creates the X_batch data\n",
    "        for sents in X_start:\n",
    "            sents = list(sents)\n",
    "            if req_length >= 7000:\n",
    "                break\n",
    "            for token in sents:\n",
    "                if req_length >= 7000:\n",
    "                    break\n",
    "                req_length += 1\n",
    "                try:\n",
    "                    new_input = w_t_i[token]\n",
    "                except:\n",
    "                    new_input = w_t_i[\"UNK\"]\n",
    "                X_batch.append(new_input)\n",
    "        while req_length < 7000:\n",
    "            req_length += 1\n",
    "            X_batch.append(0)\n",
    "        # creates the y_true value\n",
    "        y_src = y_train[index]\n",
    "        y_batch = []\n",
    "        index = 0\n",
    "        for y_sent in y_src:\n",
    "            y_sent = list(y_sent)\n",
    "            if index >= 20:\n",
    "                break\n",
    "            for token in y_sent:\n",
    "                if index >= 20:\n",
    "                    break\n",
    "                index += 1\n",
    "                try:\n",
    "                    y_batch.append(w_t_i[token])\n",
    "                except:\n",
    "                    y_batch.append(0)\n",
    "        index += 1\n",
    "        # as of now: the output is also the input to the pointer generator\n",
    "        # I know that is wrong. It should be an array that start with a special initializer token \n",
    "        # and after each step it should add the newly generated value\n",
    "        if simple:\n",
    "            yield np.array(X_batch), np.array(y_batch)\n",
    "        else:\n",
    "            yield [np.array(X_batch), np.array(y_batch)], np.array(y_batch)\n",
    "        X_start = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOAL TF-IDF to re-weight the embeddings between steps/epochs\n",
    "document_data_generator = generateStepper(X_train, y_train, trimmed_vocab, True)\n",
    "simple_model.fit(document_data_generator, steps_per_epoch = 1, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed Steps (Unfinished)\n",
    "# Actually translating to words\n",
    "#l, y = next(document_data_generator)\n",
    "#model.predict(l[0])\n",
    "# post-processing step\n",
    "# combination - get the top 200 from both and find where they overlap and use overlap. Might need to find larger sets\n",
    "#2. linear scaling - multiply together after smoothing tfidfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "1. https://machinelearningmastery.com/gentle-introduction-text-summarization/\n",
    "2. https://stackoverflow.com/questions/28373282/how-to-read-a-json-dictionary-type-file-with-pandas\n",
    "3. https://towardsdatascience.com/recurrent-neural-networks-rnns-3f06d7653a85\n",
    "4. http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html\n",
    "5. https://www.scribbr.com/apa-style/apa-abstract/\n",
    "6. https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
